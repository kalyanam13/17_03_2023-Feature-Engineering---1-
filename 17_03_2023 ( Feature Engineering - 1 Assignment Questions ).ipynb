{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1132ae",
   "metadata": {},
   "source": [
    "# PW SKILLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297ca1d",
   "metadata": {},
   "source": [
    "## Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae051c11",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486c5d7",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data for certain observations or variables. There are several reasons why data may be missing, including errors in data collection, equipment malfunctions, or intentional omissions. Handling missing values is essential for several reasons:\n",
    "\n",
    "Biased Analysis: If not addressed, missing values can lead to biased and inaccurate analysis, as the available data may not represent the true characteristics of the population.\n",
    "\n",
    "Reduced Power: Missing data can reduce the statistical power of the analysis, making it harder to detect significant effects or relationships.\n",
    "\n",
    "Misleading Results: Ignoring missing values can lead to misleading results, as the analysis may be based on incomplete or skewed information.\n",
    "\n",
    "Model Performance: Machine learning models often struggle with missing data, and their performance can be significantly impacted if not properly handled.\n",
    "\n",
    "Some algorithms that are not directly affected by missing values include:\n",
    "\n",
    "Decision Trees: Decision trees can handle missing values by selecting alternative paths during the tree-building process.\n",
    "\n",
    "Random Forests: Random Forests, being an ensemble of decision trees, also have the capability to handle missing values effectively.\n",
    "\n",
    "Naive Bayes: Naive Bayes algorithms are generally robust to missing data, although imputation methods may still be applied for better performance.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN can work with missing values by using the available features to find the nearest neighbors.\n",
    "\n",
    "Association Rule Learning: Algorithms like Apriori for association rule learning can tolerate missing values.\n",
    "\n",
    "However, it's important to note that while these algorithms may handle missing values to some extent, the quality of results can still be improved by appropriate preprocessing techniques such as imputation or data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b405b27",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data.  Give an example of each with python code.\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f9ece",
   "metadata": {},
   "source": [
    "Certainly! Here are some common techniques used to handle missing data along with examples in Python:\n",
    "\n",
    "1. Deletion of Missing Values:\n",
    "This involves removing rows or columns with missing values.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d08518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with dropped rows:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame with dropped columns:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_dropped_rows = df.dropna()\n",
    "\n",
    "# Drop columns with missing values\n",
    "df_dropped_columns = df.dropna(axis=1)\n",
    "\n",
    "print(\"DataFrame with dropped rows:\")\n",
    "print(df_dropped_rows)\n",
    "\n",
    "print(\"\\nDataFrame with dropped columns:\")\n",
    "print(df_dropped_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6adfd7c",
   "metadata": {},
   "source": [
    "2. Imputation using Mean/Median/Mode:\n",
    "Replace missing values with the mean, median, or mode of the respective column.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519428a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with mean imputation:\n",
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with the mean\n",
    "df_imputed_mean = df.fillna(df.mean())\n",
    "\n",
    "print(\"DataFrame with mean imputation:\")\n",
    "print(df_imputed_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb4691",
   "metadata": {},
   "source": [
    "3. Imputation using Forward or Backward Fill:\n",
    "Fill missing values with the previous or next valid value in the column.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2102892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with forward fill:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame with backward fill:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  4.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Forward fill missing values\n",
    "df_forward_filled = df.ffill()\n",
    "\n",
    "# Backward fill missing values\n",
    "df_backward_filled = df.bfill()\n",
    "\n",
    "print(\"DataFrame with forward fill:\")\n",
    "print(df_forward_filled)\n",
    "\n",
    "print(\"\\nDataFrame with backward fill:\")\n",
    "print(df_backward_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efec513",
   "metadata": {},
   "source": [
    "4. Imputation using Machine Learning Models:\n",
    "Train a machine learning model to predict missing values based on other features.\n",
    "Example (using scikit-learn's KNNImputer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6faa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with KNN imputation:\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.5\n",
      "2  2.5  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed_knn = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"DataFrame with KNN imputation:\")\n",
    "print(df_imputed_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfac550",
   "metadata": {},
   "source": [
    "These are just a few examples, and the choice of method depends on the nature of the data and the problem at hand. Always consider the implications of each method on the analysis or model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923644b",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c48c7",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in a classification problem where the distribution of the classes is not equal; one class significantly outnumbers the other(s). For example, in a binary classification task, if Class A has 90% of the instances and Class B has only 10%, the data is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several issues:\n",
    "\n",
    "Biased Model Performance: Machine learning models trained on imbalanced data may become biased towards the majority class. The model tends to predict the majority class more often, as it achieves a higher accuracy simply by predicting the dominant class.\n",
    "\n",
    "Poor Generalization: Models trained on imbalanced data may not generalize well to new, unseen data, especially for the minority class. The model may struggle to correctly predict instances of the minority class due to lack of exposure during training.\n",
    "\n",
    "Misleading Evaluation Metrics: Accuracy alone becomes a misleading metric for model evaluation in imbalanced datasets. Even a naive model that always predicts the majority class could achieve high accuracy, but it fails to capture the true performance, especially for the minority class.\n",
    "\n",
    "Model Sensitivity to Imbalance: Some machine learning algorithms are sensitive to class imbalance. For example, in a decision tree, if one class is dominant, the tree may be biased towards predicting that class in order to maximize overall accuracy.\n",
    "\n",
    "To address imbalanced data, several techniques can be employed:\n",
    "\n",
    "Resampling: This involves either oversampling the minority class, undersampling the majority class, or a combination of both.\n",
    "Synthetic Data Generation: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be used to generate synthetic examples of the minority class.\n",
    "Different Algorithms: Choosing algorithms that are less sensitive to class imbalance, such as ensemble methods like Random Forests or algorithms designed specifically for imbalanced data.\n",
    "Handling imbalanced data is crucial to ensure that the machine learning model gives fair and accurate predictions across all classes, rather than being biased towards the majority class. It helps in creating a more robust and reliable model for real-world applications.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233f492",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49fa7e0",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to address the issue of class imbalance in a dataset:\n",
    "\n",
    "Up-sampling (Over-sampling):\n",
    "In up-sampling, the minority class is artificially increased by generating synthetic samples or by replicating existing minority class instances.\n",
    "This is done to balance the class distribution and provide the model with more information about the minority class.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd05d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after up-sampling:\n",
      "0    7\n",
      "1    7\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample DataFrame with imbalanced classes\n",
    "data = {'Class': [1, 0, 1, 0, 0, 0, 1, 0, 0, 0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df['Class'] == 0]\n",
    "minority_class = df[df['Class'] == 1]\n",
    "\n",
    "# Up-sample the minority class\n",
    "minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
    "\n",
    "# Combine the up-sampled minority class with the majority class\n",
    "df_upsampled = pd.concat([majority_class, minority_upsampled])\n",
    "\n",
    "print(\"DataFrame after up-sampling:\")\n",
    "print(df_upsampled['Class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20fe1c2",
   "metadata": {},
   "source": [
    "Down-sampling (Under-sampling):\n",
    "In down-sampling, the majority class is reduced by randomly removing instances, creating a more balanced dataset.\n",
    "This is done to prevent the model from being biased towards the majority class.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27fdf17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after down-sampling:\n",
      "1    3\n",
      "0    3\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample DataFrame with imbalanced classes\n",
    "data = {'Class': [1, 0, 1, 0, 0, 0, 1, 0, 0, 0]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df['Class'] == 0]\n",
    "minority_class = df[df['Class'] == 1]\n",
    "\n",
    "# Down-sample the majority class\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "\n",
    "# Combine the down-sampled majority class with the minority class\n",
    "df_downsampled = pd.concat([minority_class, majority_downsampled])\n",
    "\n",
    "print(\"DataFrame after down-sampling:\")\n",
    "print(df_downsampled['Class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d53c0",
   "metadata": {},
   "source": [
    "When to use Up-sampling and Down-sampling:\n",
    "\n",
    "Up-sampling (Over-sampling):\n",
    "\n",
    "Use when the amount of data in the minority class is limited, and generating synthetic samples or replicating existing ones can help improve the model's performance.\n",
    "Useful when the minority class is under-represented and needs more attention from the model.\n",
    "Down-sampling (Under-sampling):\n",
    "\n",
    "Use when the majority class has a large number of instances, and reducing its size can help prevent the model from being biased towards it.\n",
    "Useful when there is a significant class imbalance, and the majority class overwhelms the training process.\n",
    "The choice between up-sampling and down-sampling depends on the specific characteristics of the dataset and the problem at hand. It's often a good practice to experiment with both techniques and evaluate their impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69d74d",
   "metadata": {},
   "source": [
    "### Q5: What is data Augmentation? Explain SMOTE.\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e340402",
   "metadata": {},
   "source": [
    "Data Augmentation:\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations to the existing data. These transformations include rotations, translations, scaling, flips, and other operations that create new variations of the original data. Data augmentation is commonly used in computer vision tasks, such as image classification, to enhance the diversity of the training dataset and improve the generalization ability of machine learning models.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "SMOTE is a specific data augmentation technique designed to address the class imbalance problem in classification tasks. It focuses on the minority class by generating synthetic examples to balance the class distribution. SMOTE works by creating synthetic instances along the line segments connecting existing minority class instances.\n",
    "\n",
    "Here's a step-by-step explanation of how SMOTE works:\n",
    "\n",
    "Select a Minority Instance: Randomly choose an instance from the minority class.\n",
    "\n",
    "Find Neighbors: Identify the k-nearest neighbors of the selected instance. The value of k is a user-defined parameter.\n",
    "\n",
    "Create Synthetic Instances: For each neighbor, generate synthetic instances along the line segment connecting the selected instance and its neighbor. The number of synthetic instances created depends on a specified oversampling ratio.\n",
    "\n",
    "Repeat: Repeat the process until the desired balance between classes is achieved.\n",
    "\n",
    "The goal of SMOTE is to ensure that the decision boundaries in the feature space are not biased towards the majority class, allowing the model to better capture the characteristics of the minority class.\n",
    "\n",
    "Example using the imbalanced-learn library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4b953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "1    900\n",
      "0    100\n",
      "dtype: int64\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "1    900\n",
      "0    900\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=42)\n",
    "\n",
    "# Before applying SMOTE\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# After applying SMOTE\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ba8b0",
   "metadata": {},
   "source": [
    "In this example, SMOTE is applied to balance the class distribution in a synthetic dataset, and the class distribution is printed before and after applying SMOTE to demonstrate its effectiveness in addressing class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e838c5",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe8a68",
   "metadata": {},
   "source": [
    "Outliers in a Dataset:\n",
    "Outliers are data points that deviate significantly from the rest of the data in a dataset. They are observations that lie an abnormal distance away from other values, and they can occur in one or more dimensions. Outliers can be the result of measurement errors, natural variability, or may indicate important information about the underlying process. Identifying and handling outliers is crucial in data analysis and modeling.\n",
    "\n",
    "Why it is essential to handle outliers:\n",
    "\n",
    "Impact on Descriptive Statistics: Outliers can significantly skew the summary statistics of a dataset, such as the mean and standard deviation, leading to inaccurate characterizations of the data.\n",
    "\n",
    "Model Performance: Outliers can have a disproportionate impact on certain types of models, especially those sensitive to extreme values. For example, linear regression models can be heavily influenced by outliers, leading to biased parameter estimates.\n",
    "\n",
    "Assumption Violation: Some statistical techniques and machine learning algorithms assume that the data is normally distributed or follows a certain pattern. Outliers can violate these assumptions, affecting the validity of the analysis.\n",
    "\n",
    "Data Visualization: Outliers can distort data visualization, making it challenging to create accurate and meaningful graphs and charts. Identifying and handling outliers is essential for creating reliable visualizations.\n",
    "\n",
    "Robustness of Models: Outliers can affect the robustness and generalization ability of machine learning models. Models trained on datasets with outliers may perform poorly on new, unseen data.\n",
    "\n",
    "Common Techniques for Handling Outliers:\n",
    "\n",
    "Identification and Removal: Identify outliers using statistical methods (e.g., Z-scores) or visualization tools (e.g., box plots) and remove or transform them if necessary.\n",
    "\n",
    "Transformation: Apply mathematical transformations, such as log transformation, to reduce the impact of outliers on the data.\n",
    "\n",
    "Imputation: Impute outliers with more typical values based on statistical methods or replace them with missing values for further handling.\n",
    "\n",
    "Winsorizing: Replace extreme values with values closer to the mean or within a specified range.\n",
    "\n",
    "Model-Based Approaches: Use robust models or algorithms less sensitive to outliers.\n",
    "\n",
    "Separate Analysis: Analyze the data with and without outliers to understand their impact on the results.\n",
    "\n",
    "Handling outliers is a crucial step in the data preprocessing phase to ensure the reliability and accuracy of analyses and models. The specific method chosen depends on the nature of the data and the goals of the analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da607c8",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b2af2",
   "metadata": {},
   "source": [
    "Handling missing data is a critical step in the analysis of customer data to ensure that the results are accurate and unbiased. Here are some techniques you can use to handle missing data:\n",
    "\n",
    "Deletion of Missing Values:\n",
    "\n",
    "Remove rows or columns containing missing values.\n",
    "Use cautiously, as it may lead to loss of valuable information.\n",
    "Example (deleting rows with missing values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8bea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "df_without_missing = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac19c47",
   "metadata": {},
   "source": [
    "Imputation using Mean/Median/Mode:\n",
    "\n",
    "Replace missing values with the mean, median, or mode of the respective column.\n",
    "Suitable for numerical data.\n",
    "Example (imputing with mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927001a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_mean = df.fillna(df.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee13882",
   "metadata": {},
   "source": [
    "Imputation using Forward or Backward Fill:\n",
    "\n",
    "Fill missing values with the previous or next valid value in the column.\n",
    "Suitable for time-series data.\n",
    "Example (forward fill):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647673cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forward_filled = df.ffill()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74166813",
   "metadata": {},
   "source": [
    "Imputation using Machine Learning Models:\n",
    "\n",
    "Train a machine learning model to predict missing values based on other features.\n",
    "Example (using scikit-learn's KNNImputer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d093826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed_knn = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d772d6",
   "metadata": {},
   "source": [
    "Missing Value Indicators:\n",
    "\n",
    "Introduce binary indicator variables to denote whether a value was missing.\n",
    "Useful when the fact of data being missing carries information.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature_missing'] = df['feature'].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf824b",
   "metadata": {},
   "source": [
    "Predictive Modeling for Imputation:\n",
    "\n",
    "Use regression models to predict missing values based on other features.\n",
    "Especially useful when there is a complex relationship between variables.\n",
    "Example (using scikit-learn's LinearRegression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming 'target_feature' is the feature with missing values\n",
    "known_data = df.dropna(subset=['target_feature'])\n",
    "unknown_data = df[df['target_feature'].isnull()]\n",
    "\n",
    "X_train = known_data.drop('target_feature', axis=1)\n",
    "y_train = known_data['target_feature']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted_values = model.predict(unknown_data.drop('target_feature', axis=1))\n",
    "df.loc[df['target_feature'].isnull(), 'target_feature'] = predicted_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ecd836",
   "metadata": {},
   "source": [
    "Remember to choose the appropriate technique based on the nature of your data, the reason for missingness, and the impact on your analysis or model. It's often a good practice to compare the results obtained using different imputation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff3eec",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445c1a9",
   "metadata": {},
   "source": [
    "Determining whether missing data is missing at random (MAR) or if there is a pattern to the missing data involves investigating the relationship between the missingness and the observed data. Here are some strategies to explore the nature of missing data:\n",
    "\n",
    "Descriptive Statistics:\n",
    "\n",
    "Examine summary statistics and descriptive measures for both the variables with missing data and those without missing data.\n",
    "Compare means, medians, and other statistics to identify any significant differences.\n",
    "Missing Data Heatmap:\n",
    "\n",
    "Create a heatmap or a matrix that visualizes missing values in the dataset.\n",
    "Observe if there are patterns or correlations between missing values in different variables.\n",
    "Example using Python's Seaborn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f19f228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b1110",
   "metadata": {},
   "source": [
    "Correlation Analysis:\n",
    "\n",
    "Explore correlations between missingness in one variable and the presence or absence of other variables.\n",
    "Calculate correlation coefficients or use visualizations to identify patterns.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a32e92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703cb4e",
   "metadata": {},
   "source": [
    "Missingness Tests:\n",
    "\n",
    "Conduct statistical tests to evaluate the relationship between missingness and other variables.\n",
    "Chi-square tests or Fisher's exact tests can be applied for categorical variables, while t-tests or ANOVA may be used for numerical variables.\n",
    "Pattern Recognition:\n",
    "\n",
    "Look for specific patterns in the missing data, such as consecutive missing values, clustering, or recurring sequences.\n",
    "These patterns may provide insights into the reasons behind missing data.\n",
    "Data Exploration and Visualization:\n",
    "\n",
    "Explore the data visually to identify any patterns or trends associated with missing values.\n",
    "Scatter plots, histograms, or other visualization techniques can be useful.\n",
    "Domain Knowledge:\n",
    "\n",
    "Leverage domain knowledge to understand whether the missingness is reasonable given the context of the data.\n",
    "Consult with subject matter experts to gain insights into potential reasons for missing values.\n",
    "Impute and Compare:\n",
    "\n",
    "Impute missing values using different techniques and compare the results.\n",
    "If imputed values differ significantly from observed values, it may indicate a non-random pattern in the missing data.\n",
    "By employing these strategies, you can gain a better understanding of whether missing data is missing at random or if there is a discernible pattern. Keep in mind that it might be challenging to definitively determine the reason for missing data, but these approaches can provide valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f85a3a",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6722a84",
   "metadata": {},
   "source": [
    "Working with imbalanced datasets, where one class is significantly more prevalent than the other, requires careful consideration to ensure that the machine learning model is effectively trained and evaluated. Here are some strategies to evaluate the performance of your machine learning model on an imbalanced dataset in a medical diagnosis project:\n",
    "\n",
    "Use Appropriate Evaluation Metrics:\n",
    "\n",
    "Avoid relying solely on accuracy, as it can be misleading in imbalanced datasets. Instead, use metrics such as precision, recall, F1 score, and area under the Receiver Operating Characteristic (ROC) curve.\n",
    "Precision: \n",
    "True Positives\n",
    "True Positives + False Positives\n",
    "True Positives + False Positives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "Recall: \n",
    "True Positives\n",
    "True Positives + False Negatives\n",
    "True Positives + False Negatives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "F1 Score: \n",
    "2\n",
    "×\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision + Recall\n",
    "2× \n",
    "Precision + Recall\n",
    "Precision×Recall\n",
    "​\n",
    " \n",
    "Confusion Matrix Analysis:\n",
    "\n",
    "Examine the confusion matrix to understand the distribution of true positives, true negatives, false positives, and false negatives.\n",
    "Gain insights into where the model excels or struggles.\n",
    "ROC Curve and AUC-ROC:\n",
    "\n",
    "Plot the Receiver Operating Characteristic (ROC) curve and calculate the Area Under the Curve (AUC-ROC).\n",
    "ROC curves provide a visual representation of the trade-off between sensitivity and specificity.\n",
    "Precision-Recall Curve and AUC-PR:\n",
    "\n",
    "Create a Precision-Recall curve and calculate the Area Under the Curve (AUC-PR).\n",
    "Especially useful when the positive class is rare.\n",
    "Class Weights:\n",
    "\n",
    "Assign higher weights to the minority class during model training to make the model more sensitive to it.\n",
    "Some machine learning libraries, such as scikit-learn, allow setting class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming 'X' and 'y' are your features and target variable\n",
    "model = RandomForestClassifier(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b3622",
   "metadata": {},
   "source": [
    "Resampling Techniques:\n",
    "\n",
    "Apply resampling techniques like over-sampling the minority class or under-sampling the majority class.\n",
    "Methods like SMOTE (Synthetic Minority Over-sampling Technique) can generate synthetic samples for the minority class.\n",
    "Ensemble Models:\n",
    "\n",
    "Use ensemble models, such as Random Forests or Gradient Boosting, which can handle imbalanced datasets more effectively.\n",
    "Threshold Adjustment:\n",
    "\n",
    "Adjust the classification threshold to balance precision and recall based on the specific requirements of the application.\n",
    "Higher threshold increases precision but may reduce recall, and vice versa.\n",
    "Stratified Cross-Validation:\n",
    "\n",
    "Employ stratified cross-validation to ensure that each fold maintains the original class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bdab3",
   "metadata": {},
   "source": [
    "Anomaly Detection Techniques:\n",
    "\n",
    "If applicable, consider treating the minority class as anomalies and use anomaly detection techniques.\n",
    "Cost-sensitive Learning:\n",
    "\n",
    "Introduce cost-sensitive learning by assigning different misclassification costs to different classes.\n",
    "Keep in mind that the choice of strategy depends on the specific characteristics of your dataset and the goals of the medical diagnosis project. Experimenting with multiple approaches and carefully evaluating the performance metrics will help in selecting the most suitable strategy for your particular use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f34cb2",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c99c2c",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset in the context of estimating customer satisfaction, where the majority class (e.g., satisfied customers) significantly outweighs the minority class (e.g., dissatisfied customers), down-sampling the majority class can be a useful strategy to balance the dataset. Here are several methods you can employ to down-sample the majority class:\n",
    "\n",
    "Random Under-Sampling:\n",
    "\n",
    "Randomly remove instances from the majority class to match the size of the minority class.\n",
    "This approach may lead to loss of information, so it's essential to assess its impact on model performance.\n",
    "Cluster Centroids:\n",
    "\n",
    "Use the cluster centroids technique, which involves clustering the majority class instances and then creating new instances at the centroids of these clusters.\n",
    "This method aims to maintain the representativeness of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "cc = ClusterCentroids(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f57cc",
   "metadata": {},
   "source": [
    "NearMiss:\n",
    "\n",
    "The NearMiss algorithm selects instances from the majority class that are close to the minority class instances based on distance metrics.\n",
    "It can be helpful in preserving the information of the majority class while balancing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "nm = NearMiss(sampling_strategy='auto', version=2, random_state=42)\n",
    "X_resampled, y_resampled = nm.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246edec9",
   "metadata": {},
   "source": [
    "Tomek Links:\n",
    "\n",
    "Identify Tomek links, which are pairs of instances (one from the majority class and one from the minority class) that are close to each other but have different class labels.\n",
    "Remove the majority class instance in each Tomek link to create a more balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='auto')\n",
    "X_resampled, y_resampled = tl.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a4d5a",
   "metadata": {},
   "source": [
    "Edited Nearest Neighbors (ENN):\n",
    "\n",
    "ENN removes instances from the majority class that are misclassified by their k-nearest neighbors.\n",
    "This method aims to improve the decision boundary between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "X_resampled, y_resampled = enn.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8836cf2",
   "metadata": {},
   "source": [
    "Combining Under-Sampling and Over-Sampling:\n",
    "\n",
    "Apply a combination of under-sampling for the majority class and over-sampling for the minority class to achieve a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952a079",
   "metadata": {},
   "source": [
    "Before deciding on a specific method, it's important to experiment with different approaches and evaluate their impact on the performance of your customer satisfaction estimation model. Additionally, you may want to use cross-validation to assess the generalization ability of the model with the down-sampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2bdcb",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7d6ee",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset, especially in scenarios where there is a rare event (minority class), up-sampling the minority class can be beneficial to ensure that the machine learning model adequately learns from the under-represented class. Here are several methods you can employ to up-sample the minority class:\n",
    "\n",
    "Random Over-Sampling:\n",
    "\n",
    "Randomly duplicate instances from the minority class to increase its size.\n",
    "Be cautious about overfitting, as this method may lead to the model memorizing the minority class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdee884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd24fe0",
   "metadata": {},
   "source": [
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "Generate synthetic instances for the minority class using interpolation techniques.\n",
    "SMOTE aims to create synthetic examples along the line segments connecting existing minority class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ab7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abde32b",
   "metadata": {},
   "source": [
    "ADASYN (Adaptive Synthetic Sampling):\n",
    "\n",
    "Similar to SMOTE, but it adaptively adjusts the weights given to different minority class instances, making the generation of synthetic examples more focused on difficult-to-learn instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbe13d",
   "metadata": {},
   "source": [
    "SMOTENC:\n",
    "\n",
    "An extension of SMOTE that supports categorical features.\n",
    "Useful when dealing with datasets that contain a combination of numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed147ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# Assuming 'categorical_features' is a list of indices of categorical features\n",
    "smotenc = SMOTENC(sampling_strategy='auto', categorical_features=categorical_features, random_state=42)\n",
    "X_resampled, y_resampled = smotenc.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569e844",
   "metadata": {},
   "source": [
    "Borderline-SMOTE:\n",
    "\n",
    "Focuses on generating synthetic examples near the decision boundary between classes.\n",
    "It aims to address instances that are difficult to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98413a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "borderline_smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = borderline_smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9074352",
   "metadata": {},
   "source": [
    "SVMSMOTE:\n",
    "\n",
    "Uses support vector machines to identify instances that are difficult to classify and generates synthetic examples in these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SVMSMOTE\n",
    "\n",
    "svm_smote = SVMSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = svm_smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1e204",
   "metadata": {},
   "source": [
    "It's important to note that while up-sampling methods can address class imbalance, they also introduce some level of noise, and the effectiveness of each method may vary depending on the specific characteristics of the dataset. Experimentation and careful evaluation of model performance are crucial to finding the most suitable approach for a given project.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f0fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429ab1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
